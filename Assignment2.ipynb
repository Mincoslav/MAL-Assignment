{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning</h1><h2 align=\"center\" style=\"margin:10px\">Assignment 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student names and numbers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignments below should be solved and documented as a mini-project that will form the basis for the\n",
    "examination. When solving the exercises it is important that you\n",
    "\n",
    "  * document all relevant results and analyses that you have obtained/performed during the exercises\n",
    "  * try to relate your results to the theoretical background of the methods being applied.\n",
    "\n",
    "Feel free to add cells if you need to. The easiest way to convert to pdf is to save this notebook as .html (File-->Download as-->HTML) and then convert this html file to pdf. You can also export as pdf directly, but here you need to watch your margins as the converter will cut off your code (i.e. make vertical code!).\n",
    "\n",
    "Last, but not least:\n",
    "* Looking for an overview of the markdown language? The cheat sheet <a href=\"https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\">here</a> might help.\n",
    "* For most of the Python specific components of the exercises, you should not need constructs beyond those that are already included in the notebooks on the course's web-page (still you should not feel constrained by these, so feel free to be adventurous). You may, however, need to consult the documentation for some of the methods supplied by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\spiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\spiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries here\n",
    "import mglearn\n",
    "import sys\n",
    "import nltk\n",
    "import string \n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy.random import seed\n",
    "from keras.datasets import mnist\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras import optimizers\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "np.set_printoptions(threshold= sys.maxsize)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDB-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('IMDB_dataset/reviews.txt', header=None)\n",
    "labels = pd.read_csv('IMDB_dataset/labels.txt', header=None)\n",
    "Y = np.array((labels=='positive').astype(np.int_)).ravel()\n",
    "# print(type(reviews))\n",
    "# print(reviews[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing.\n",
    "\n",
    "**b)** Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. (See an example of how to do this in chapter 7 of \"Muller and Guido\"). Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`).\n",
    "\n",
    "**c)** Explore the representation of the reviews. How is a single word represented? How about a whole review?\n",
    "\n",
    "**d)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. \n",
    "\n",
    "**e)** Test your sentiment-classifier on the test set.\n",
    "\n",
    "**f)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1 Solution***\n",
    "\n",
    "Having to work with text data posed the first problem for coming out with a good model solution for the exercise. Text data is often viewed as an interpretation of sentiment, which is unquantifiable by a computer. \n",
    "\n",
    "The exercise formulation mentions as requirement the use of a CountVectorizer in the idea of creating a Bag of words, essentially mapping each word of a review to a tuple (key and value) and thus transforming a potential review in an array of tuples. In this case we will attempt creating a new dataset from the one provided where its features will then be represented by the keys contained in the bag of words, and the values will be the actual \"data\" property in the data set.\n",
    "\n",
    "To construct the dataset, first the information has to go through a pre-processing step as creating a bag of words could potentially tokenize frequent words that do not necessarily influence or characterize the main \"sentiment\" of a review. For the exercise in question, the following process was proposed:\n",
    "\n",
    "**1.** Removal of stop_words: <br>\n",
    "This step describes the removal of all stop words from the reviews. Example of stop words are 'i', 'am', ''t' etc.\n",
    "\n",
    "**2.** Punctuation removal: <br>\n",
    "By creating a bag of words, punctuation is considered a word, or a token itself. Due to the fact that they do not pose a significant value in predicting the affirmativeness of a review, they are removed.\n",
    "\n",
    "**3.** Lemmatisation of reviews: <br>\n",
    "Lemmatisation is the process of replacing a word (which presents a specific form depending of its type and the grammar implications of the language) with its prime form that can be found in a dictionary. Like so verbs, nouns or adjectives (didn't, coded, affirmativeness) are transposed (do, code, affirmative). This reduces redundancy in the data set.\n",
    "\n",
    "<br>\n",
    "In typical use cases we would have to split our training data from the testing data and perform the pre-processing procedure only on the training data, but due to the fact that we are essentially creating a new data set from the one provided, and the test data has to be reduces to a form which the model ca understand, we will process all reviews at once and then split. \n",
    "<br>\n",
    "Due to the requirement of using a neural network, we will want to calculate the probability of a review to be negative of positive. In this case, the problem resumes to a logistic regression issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Removal of stop_words\n",
    "for idx, rev in enumerate(reviews[0]):\n",
    "    reviews[0][idx] = remove_stopwords(rev, stopwords.words('english') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                        0\n",
      "0      bromwell high cartoon comedy . ran time progra...\n",
      "1      story man unnatural feelings pig . starts open...\n",
      "2      homelessness houselessness george carlin state...\n",
      "3      airport starts brand new luxury plane loaded v...\n",
      "4      brilliant acting lesley ann warren . best dram...\n",
      "...                                                  ...\n",
      "24995  saw descent last night stockholm film festival...\n",
      "24996  christmas together actually came time raised j...\n",
      "24997  films pick pound turn rather good rd century f...\n",
      "24998  working class romantic drama director martin r...\n",
      "24999  one dumbest films ever seen . rips nearly ever...\n",
      "\n",
      "[25000 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "#The result of the removal of stop words can be seen by printing the first few reviews \n",
    "print(reviews.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Punctuation removal\n",
    "for idx, rev in enumerate(reviews[0]):\n",
    "    reviews[0][idx] = rev.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                        0\n",
      "0      bromwell high cartoon comedy  ran time program...\n",
      "1      story man unnatural feelings pig  starts openi...\n",
      "2      homelessness houselessness george carlin state...\n",
      "3      airport starts brand new luxury plane loaded v...\n",
      "4      brilliant acting lesley ann warren  best drama...\n",
      "...                                                  ...\n",
      "24995  saw descent last night stockholm film festival...\n",
      "24996  christmas together actually came time raised j...\n",
      "24997  films pick pound turn rather good rd century f...\n",
      "24998  working class romantic drama director martin r...\n",
      "24999  one dumbest films ever seen  rips nearly ever ...\n",
      "\n",
      "[25000 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "# The result of the removal of punctuation can be seen by printing the first few reviews \n",
    "print(reviews.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Lemmatisation of reviews \n",
    "lmtizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    " \n",
    " \n",
    "for idx, rev in enumerate(reviews[0]):\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(rev)) \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:  \n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:  \n",
    "            lemmatized_sentence.append(lmtizer.lemmatize(word, tag))\n",
    "    reviews[0][idx]= \" \".join(lemmatized_sentence)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is meant to lemmatize each word in each reviews included in the initial data set. The lemmatisation process depends on the type of word that needs to be lemmatised. Using the `nltk.pos_tag` function we can extract the type (or tag) of all words from a review. The nltk tags are quite hard to read, such that they were mapped into a list of more clear options using the defined function in the above code. Using the list of words from a review and their corresponding tag, the review is reconstructed using the results of the lemmatized process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high cartoon comedy run time program school life teacher year teach profession lead believe bromwell high satire much closer reality teacher scramble survive financially insightful student see right pathetic teacher pomp pettiness whole situation remind school know student saw episode student repeatedly try burn school immediately recall high classic line inspector sack one teacher student welcome bromwell high expect many adult age think bromwell high far fetch pity\n"
     ]
    }
   ],
   "source": [
    "# The result of the lemmatisation process on the first review is the following\n",
    "print(reviews[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Having the reviews run through the pre-processing step, the result represents a very concrete, direct message. While some of the nuance of the review is lost, the message becomes more compact and concentrated and less redundant in terms of content. Having this form the reviews can be used to create the bag of words. \n",
    "\n",
    "The solution presented in this report makes use of a `TfidfVectorizer` instead of `CountVectorizer`. A `TfidfVectorizer` is an implementation of a `CountVectorizer`, meaning that they output the same ´vocabulary_´, their difference being the value they give when performing transformation of a document (review). A CountVectorizer converts a collection of text documents to a matrix of token counts, meaning that the result of a review transformation will contain the number of times a specific word (in the respective review) appears. TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency, meaning that when token counts are calculated, they are computed and considered across the entire document (data set) and outputs to a frequency, a ratio or weight of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Tokenizing words\n",
    "vect = TfidfVectorizer(max_features=10000, stop_words= stopwords.words('english'))\n",
    "transformed_reviews = vect.fit_transform(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** The result of the transformation is also the dataset we will use to train the neural network on. To read the transformed reviews (a sparse matrix) we will consider the following:\n",
    "\n",
    "* A row in the matrix corresponds to the output of the transformation of a document for one specific word which is included in the bag of words (one of the most 10000 frequent words)\n",
    "* The first element of the tuple describes to the review for which the output corresponds. 0 means the first review, 1 the second, 2 the thirds, etc. \n",
    "* The second element of the tuple corresponds the index of the word in the bag of words. For example the value 6710 corresponds to the 6710'th most frequent word from the 'vocabulary_' which is ´pity´. If a specific index appears in a tuple, that means that the word it mapped to is included in the review corresponding to the first value in the tuple.\n",
    "* The value to which the tuple maps corresponds to the frequency the word has in the entire data set. This is also how the words will be represented in the final data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6710)\t0.11950034363725918\n",
      "  (0, 3337)\t0.137302799335336\n",
      "  (0, 3261)\t0.06932324862812123\n",
      "  (0, 8969)\t0.04584721706500551\n",
      "  (0, 180)\t0.08110951562857681\n",
      "  (0, 136)\t0.09614733214310987\n",
      "  (0, 5515)\t0.05490404589168599\n",
      "  (0, 3151)\t0.07323714228205612\n",
      "  (0, 9748)\t0.11804706361208209\n",
      "  (0, 6291)\t0.03284128128186592\n",
      "  (0, 7692)\t0.15057204889412243\n",
      "  (0, 4618)\t0.1347889174185391\n",
      "  (0, 5255)\t0.0673397466196941\n",
      "  (0, 1578)\t0.07749010060053112\n",
      "  (0, 7250)\t0.11747651616388127\n",
      "  (0, 4450)\t0.10683414307557462\n",
      "  (0, 1171)\t0.10737584204656342\n",
      "  (0, 9243)\t0.05621233858721317\n",
      "  (0, 7395)\t0.13442737483054762\n",
      "  (0, 3003)\t0.08405752000690257\n",
      "  (0, 7772)\t0.06771878411874818\n",
      "  (0, 5022)\t0.04942392242584494\n",
      "  (0, 7370)\t0.09638331480183294\n",
      "  (0, 8155)\t0.08908138293337045\n",
      "  (0, 9789)\t0.068673186657198\n",
      "  :\t:\n",
      "  (24999, 5328)\t0.06374111525662078\n",
      "  (24999, 97)\t0.08299198568842231\n",
      "  (24999, 7774)\t0.12833026635764289\n",
      "  (24999, 8995)\t0.12274689379855375\n",
      "  (24999, 2456)\t0.09882551826074257\n",
      "  (24999, 5295)\t0.15291466007104798\n",
      "  (24999, 5861)\t0.0984990724522427\n",
      "  (24999, 3367)\t0.08445011474010568\n",
      "  (24999, 6749)\t0.07129520283547786\n",
      "  (24999, 3742)\t0.05147951486858716\n",
      "  (24999, 1429)\t0.058568274258403566\n",
      "  (24999, 2206)\t0.08482932872704538\n",
      "  (24999, 5231)\t0.09513017140039177\n",
      "  (24999, 1033)\t0.0853309771807012\n",
      "  (24999, 6585)\t0.06630793791935051\n",
      "  (24999, 3800)\t0.05620851071508977\n",
      "  (24999, 3823)\t0.10331957163037638\n",
      "  (24999, 5470)\t0.046080523475546996\n",
      "  (24999, 3261)\t0.09120513272147411\n",
      "  (24999, 8969)\t0.06031889157063853\n",
      "  (24999, 6291)\t0.04320763203299727\n",
      "  (24999, 5255)\t0.08859553828509159\n",
      "  (24999, 9789)\t0.09034987868320354\n",
      "  (24999, 7866)\t0.0476489218122226\n",
      "  (24999, 9032)\t0.1051310884158236\n"
     ]
    }
   ],
   "source": [
    "print(transformed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9     ...  9990  \\\n",
       "0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "24995   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "24996   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "24997   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "24998   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "24999   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "       9991  9992  9993      9994  9995  9996  9997  9998  9999  \n",
       "0       0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "1       0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "3       0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "4       0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...   ...   ...       ...   ...   ...   ...   ...   ...  \n",
       "24995   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "24996   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "24997   0.0   0.0   0.0  0.198158   0.0   0.0   0.0   0.0   0.0  \n",
       "24998   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "24999   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[25000 rows x 10000 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For visualization purposes, the dataset (the tranformed reviews) can be plotted in a data frame.\n",
    "dataFrame = pd.DataFrame.sparse.from_spmatrix(transformed_reviews)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Splitting the training, validation and test is represented by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training-val set:18750\n",
      "Size of test set:6250\n"
     ]
    }
   ],
   "source": [
    "Y = to_categorical(Y,num_classes=2)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(dataFrame.to_numpy(), Y, random_state=42)\n",
    "print(\"Size of training-val set:{}\".format(X_trainval.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** The code below describes the network configuration and the fitting process of the data in the model.\n",
    "Observations regarding the neuron network:\n",
    "*  The validation methodology was applied by mentioning the `validation_split` parameter when fitting the training data. As such, the network performs validations on 0.2 out of bach set on which the network learn at each one epoch. \n",
    "*  It was discovered that having a great number of neurons on the input and hidden layer caused the network to look 'to deep' into the data, causing great loss in the validation process, thus overfitting.\n",
    "* `Relu` was used as activation algorithm as the data was previously stripped of potential noise that could have been treated with `Sigmoid` activation. \n",
    "* In general the score of the network comes down to providing a proportional value for the epoch and batch_size. The more we increase the batch_size the more epochs we would like to input. In general it was discovered that 0.2 as the validation split ratio gave the best result, as more would cut to much from the training data, but to little (while increasing the accuracy with even 0.04) caused the training loss to be with even 0.1 smaller then the validation loss, thus indicating that the model is overfitting.\n",
    "* loss: 0.2291 - accuracy: 0.9113 - val_loss: 0.2819 - val_accuracy: 0.8864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "150/150 [==============================] - 2s 13ms/step - loss: 0.6888 - accuracy: 0.5622 - val_loss: 0.6835 - val_accuracy: 0.4971\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6635 - accuracy: 0.6999 - val_loss: 0.6363 - val_accuracy: 0.7491\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5770 - accuracy: 0.7848 - val_loss: 0.5530 - val_accuracy: 0.6957\n",
      "Epoch 4/15\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.4923 - accuracy: 0.7641 - val_loss: 0.5669 - val_accuracy: 0.6856\n",
      "Epoch 5/15\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.4688 - accuracy: 0.7641 - val_loss: 0.3927 - val_accuracy: 0.8432\n",
      "Epoch 6/15\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.3905 - accuracy: 0.8242 - val_loss: 0.3555 - val_accuracy: 0.8539\n",
      "Epoch 7/15\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.3511 - accuracy: 0.8469 - val_loss: 0.4087 - val_accuracy: 0.8013\n",
      "Epoch 8/15\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.3445 - accuracy: 0.8508 - val_loss: 0.3050 - val_accuracy: 0.8755\n",
      "Epoch 9/15\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.3221 - accuracy: 0.8631 - val_loss: 0.3204 - val_accuracy: 0.8667\n",
      "Epoch 10/15\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.3009 - accuracy: 0.8718 - val_loss: 0.5618 - val_accuracy: 0.7432\n",
      "Epoch 11/15\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.2864 - accuracy: 0.8825 - val_loss: 0.2885 - val_accuracy: 0.8829\n",
      "Epoch 12/15\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2568 - accuracy: 0.8944 - val_loss: 0.3158 - val_accuracy: 0.8672\n",
      "Epoch 13/15\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.2683 - accuracy: 0.8906 - val_loss: 0.3162 - val_accuracy: 0.8683\n",
      "Epoch 14/15\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.2558 - accuracy: 0.8942 - val_loss: 0.3769 - val_accuracy: 0.8403\n",
      "Epoch 15/15\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2291 - accuracy: 0.9113 - val_loss: 0.2819 - val_accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=10000)) \n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax')) \n",
    "\n",
    "sdg = optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sdg, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_trainval, y_trainval, validation_split = 0.2, epochs=15, batch_size=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** To evaluate the network regard the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 1s 3ms/step - loss: 0.1932 - accuracy: 0.9282\n",
      "Accuracy on training data: [0.19319471716880798, 0.928213357925415]\n",
      "196/196 [==============================] - 1s 3ms/step - loss: 0.2816 - accuracy: 0.8867\n",
      "Accuracy on testing data: [0.2816464900970459, 0.8867200016975403]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training data: {}\".format(model.evaluate(X_trainval, y_trainval)))\n",
    "print(\"Accuracy on testing data: {}\".format(model.evaluate(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** The reviews were taken from online sources. Before they are predicted, the reviews are taken through a processing function that corresponds to the pre-processing step elaborated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review1 has 0.4365568161010742 chances to be a negative review and 0.5634431838989258 chances to be positive\n",
      "Review2 has 0.08165528625249863 chances to be a negative review and 0.9183447360992432 chances to be positive\n"
     ]
    }
   ],
   "source": [
    "review1 = \"There were two interesting look back reviews that came out in 2015 that are worth checking out - just search for <<review terminator sarah connor chronicles>> and check out the retrospectives in The Guardian and on IGN. The IGN retrospective is especially valuable due to the inclusion of video cast interviews that don't show up on YouTube - there are a lot of insights as to how the principle actors viewed their characters and where they hoped the plot and reveals would go. Ultimately the writer Josh Friedman did give us fans what we wanted, just not in the definitive manner the actors hoped for their characters. I think it is better that way - showing us and let us draw our own conclusions, instead of laying it out in black and white. But, yeah, for you Jameron shippers, you were right, the seeds were there all along and the final episode is a real heartbreaker.\"\n",
    "review2 = \"One of the best stand alone TV Sci-Fi shows ever, joining Firefly on the list of <<why ever did they cancel that >>? Perhaps the best answer is that it is a miracle it was made in the first place, given that cancer survivor Josh Friedman produced a death obsessed, stylish, inventive and often very funny TV show with more depth and sophistication than the dying film franchise. A Hollywood insider needs to write the real back story to this show: how it suited Warner Brothers to have what was, in effect, an extended commercial on TV for the upcoming reboot of the movie franchise, what transpired between Warner and Fox, and why Fox started to pull the plug on the cash. There are episodes in season two that were clearly made with everyone concerned expecting to be fired by the end of the day. The production values dropped as the budget dried up, but the excellent acting, production and cinematography did not. An unfortunate casualty of the writers' strike, the show lost momentum after its truncated first season. Neflix, reboot it, please....\"\n",
    "\n",
    "\n",
    "def transform_review(review):\n",
    "    review = remove_stopwords(review, stopwords.words('english'))\n",
    "    review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(review)) \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:  \n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:  \n",
    "            lemmatized_sentence.append(lmtizer.lemmatize(word, tag))\n",
    "    review = \" \".join(lemmatized_sentence)\n",
    "    return review\n",
    "\n",
    "review1 = transform_review(review1)\n",
    "review1 = vect.transform([review1])\n",
    "review1 = pd.DataFrame.sparse.from_spmatrix(review1).to_numpy()\n",
    "\n",
    "review2 = transform_review(review2)\n",
    "review2 = vect.transform([review2])\n",
    "review2 = pd.DataFrame.sparse.from_spmatrix(review2).to_numpy()\n",
    "\n",
    "prediction1 = model.predict(review1)\n",
    "prediction2 = model.predict(review2)\n",
    "print(\"Review1 has {} chances to be a negative review and {} chances to be positive\".format(prediction1[0][0], prediction1[0][1]))\n",
    "print(\"Review2 has {} chances to be a negative review and {} chances to be positive\".format(prediction2[0][0], prediction2[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we consider the famous MNIST dataset, which is loaded below. The dataset consists of 70000 handwritten digits 0-9 at a resolution of 28x28 pixels. In the cell below, the dataset is loaded and split into 60000 traning and 10000 testing images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code-snippet below can be used to see the images corresponding to individual digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "plt.imshow(x_train[index],cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little bit simpler (and faster!), we can extract from the data binary subsets, that only contain the data for two selected digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first training datapoint now is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOdUlEQVR4nO3dbYyV9ZnH8d+1Y3mhgDLLoDjgTq1gipsI9QibsJEas40PMdgX3UAiYkIc4lNoJMaHjRZ8SMjGtkGyKYF1UqpdamMRJmi2NaRC+qbhoKziklXXYBkYYdDI0Fdd5doXc9uMMOd/Due+zwNzfT/J5JxzX+c/98VhfnOfOf/7nL+5uwCMf3/T6gYANAdhB4Ig7EAQhB0IgrADQVzQzJ1NnTrVe3p6mrlLIJRDhw7pxIkTNlYtV9jN7GZJ6yV1SPp3d1+Xun9PT4/K5XKeXQJIKJVKFWt1P403sw5J/ybpFklzJC01szn1fj8AjZXnb/b5kj5094/c/S+SfiVpcTFtAShanrB3Szo86vZAtu1rzKzXzMpmVh4aGsqxOwB55An7WC8CnHXurbtvcveSu5e6urpy7A5AHnnCPiBp5qjbMyQdzdcOgEbJE/a9kmaZ2TfNbIKkJZL6i2kLQNHqnnpz9y/M7AFJv9XI1Fufu79XWGcACpVrnt3dX5f0ekG9AGggTpcFgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiFyruKL97d69O1nft29fsv70008n6ydPnjznnmrl7sn6wMBAst7d3V1kO+e9XGE3s0OSTkn6UtIX7l4qoikAxSviyH6ju58o4PsAaCD+ZgeCyBt2l/Q7M9tnZr1j3cHMes2sbGbloaGhnLsDUK+8YV/o7t+RdIuk+83shjPv4O6b3L3k7qWurq6cuwNQr1xhd/ej2eVxSa9Kml9EUwCKV3fYzewiM5v01XVJ35N0oKjGABQrz6vxl0p61cy++j7/4e7/WUhX+Jq+vr5kfe3atRVr1ebBh4eHk/Xs/7fueiMtWLAgWe/o6KhYW758eXLsnXfemazPnj07WW9HdYfd3T+SdG2BvQBoIKbegCAIOxAEYQeCIOxAEIQdCGLcvMX16NGjyfrUqVOT9QkTJhTZzjnZtWtXsv7QQw8l66dOnSqynfPG4OBg3WOfffbZZL2zszNZPx+n3jiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ42ae/bXXXkvWb7/99mT9sssuK7Kdc3L11Vcn64888kiyvmbNmoq1Cy+8MDm22ltgL7nkkmS9p6cnWV+2bFnFWn9/f3Lsm2++mazncdVVVyXrt912W8P23Soc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiHEzz37PPfe0uoW6zZgxI1m/++67k/Vp06ZVrFWbT3777beT9RtuOGuRn6/ZsGFDsr569epkvZFS5wDs3LkzOXbWrFkFd9N6HNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IIhxM88+nk2fPj1ZX7FiRcXa7t27k2NPnz6drN90003JerX3w+fh7sl6b29vsj5nzpyKtfE4j15N1SO7mfWZ2XEzOzBqW6eZvWFmH2SXUxrbJoC8anka/3NJN5+x7VFJu9x9lqRd2W0Abaxq2N19j6TPzti8WNKW7PoWSXcU2xaAotX7At2l7j4oSdllxZOzzazXzMpmVh4aGqpzdwDyavir8e6+yd1L7l7q6upq9O4AVFBv2I+Z2XRJyi6PF9cSgEaoN+z9kpZn15dL2lFMOwAapeo8u5ltlfRdSVPNbEDSjyStk/RrM1sh6U+SftDIJqP79NNPk/Vt27ZVrD388MPJscPDw8m6meWqp8ydOzdZT50/IEkrV65M1js6Os61pXGtatjdfWmFUvpsCwBthdNlgSAIOxAEYQeCIOxAEIQdCIK3uJ4HHnzwwWT95ZdfblInZ6u2FPZjjz1WsXbFFVckx1Z7ay/ODUd2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCefbzwOeff97qFiqq9ulDM2fOrFhjHr25OLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBBWbVncIpVKJS+Xy03b33gxODiYrOd5P/uePXuS9R078i0JMHny5Iq17du3J8cuWrQo174jKpVKKpfLY36+N0d2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCeXYkPfXUU8n6xo0bk/VPPvmk7n0/99xzyXpvb2+yPnHixLr3fb7KNc9uZn1mdtzMDozatsbMjpjZ/uzr1iIbBlC8Wp7G/1zSzWNs/6m7z82+Xi+2LQBFqxp2d98j6bMm9AKggfK8QPeAmb2TPc2fUulOZtZrZmUzKw8NDeXYHYA86g37zyR9S9JcSYOSflzpju6+yd1L7l6q9uGEABqnrrC7+zF3/9LdT0vaLGl+sW0BKFpdYTez0Z8B/H1JByrdF0B7qDrPbmZbJX1X0lRJxyT9KLs9V5JLOiRppbun33Qt5tnHo7179ybrzzzzTMXazp07k2Or/WwePnw4We/u7k7Wx6PUPHvVRSLcfekYm1/I3RWApuJ0WSAIwg4EQdiBIAg7EARhB4JgyWbkcv311yfrmzdvrli77rrrkmOPHDmSrG/YsCFZX7duXbIeDUd2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCeXY0VEdHR8Va3o96vvbaa3ONj4YjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTw7Gmrr1q0Va++//36u793f35+sL1061gcjx8WRHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJ69Ri+99FLF2l133ZUcW23p4YGBgWQ9z9LDu3fvTtb37duXrJuNufrvXz3//PPJ+scff5ysp8ybNy9Z37hxY93fO6KqR3Yzm2lmvzezg2b2npmtyrZ3mtkbZvZBdjml8e0CqFctT+O/kLTa3b8t6R8k3W9mcyQ9KmmXu8+StCu7DaBNVQ27uw+6+1vZ9VOSDkrqlrRY0pbsblsk3dGgHgEU4JxeoDOzHknzJP1R0qXuPiiN/EKQNK3CmF4zK5tZeWhoKGe7AOpVc9jNbKKk30j6obsP1zrO3Te5e8ndS11dXfX0CKAANYXdzL6hkaD/0t23ZZuPmdn0rD5d0vHGtAigCFWn3mxk7uUFSQfd/SejSv2Slktal13uaEiHTVJtCuree++tWKs2PVXNzp07k/XOzs5k/cUXX6xY27NnT3Ls8HD6SVref1ue8atWrUrWL7744rq/d0S1zLMvlLRM0rtmtj/b9rhGQv5rM1sh6U+SftCQDgEUomrY3f0Pkir9er6p2HYANAqnywJBEHYgCMIOBEHYgSAIOxAEb3HNzJ49O1lfsmRJxVpfX1+ufd933325xrfS5MmTk/WFCxdWrD3xxBPJsQsWLKirJ4yNIzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8e2bSpEnJ+tq1ayvWXnnlleTYkydP1tVTrS64oPJ/4+WXX54ce/r06WT9ySefTNavvPLKZP3GG29M1tE8HNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjm2WuUmq/evn17cuz69euT9R070h+5X+3z06+55pqKtRUrViTHIg6O7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRC3rs8+U9AtJl0k6LWmTu683szWS7pE0lN31cXd/vVGNtrNFixblqgPNUMtJNV9IWu3ub5nZJEn7zOyNrPZTd3+uce0BKEot67MPShrMrp8ys4OSuhvdGIBindPf7GbWI2mepD9mmx4ws3fMrM/MplQY02tmZTMrDw0NjXUXAE1Qc9jNbKKk30j6obsPS/qZpG9JmquRI/+Pxxrn7pvcveTupa6urvwdA6hLTWE3s29oJOi/dPdtkuTux9z9S3c/LWmzpPmNaxNAXlXDbmYm6QVJB939J6O2Tx91t+9LOlB8ewCKUsur8QslLZP0rpntz7Y9Lmmpmc2V5JIOSVrZgP4AFKSWV+P/IMnGKIWcUwfOV5xBBwRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCMLcvXk7MxuS9PGoTVMlnWhaA+emXXtr174keqtXkb39nbuP+flvTQ37WTs3K7t7qWUNJLRrb+3al0Rv9WpWbzyNB4Ig7EAQrQ77phbvP6Vde2vXviR6q1dTemvp3+wAmqfVR3YATULYgSBaEnYzu9nM/sfMPjSzR1vRQyVmdsjM3jWz/WZWbnEvfWZ23MwOjNrWaWZvmNkH2eWYa+y1qLc1ZnYke+z2m9mtLeptppn93swOmtl7ZrYq297Sxy7RV1Met6b/zW5mHZLel/RPkgYk7ZW01N3/u6mNVGBmhySV3L3lJ2CY2Q2S/izpF+7+99m2f5X0mbuvy35RTnH3R9qktzWS/tzqZbyz1Yqmj15mXNIdku5WCx+7RF//rCY8bq04ss+X9KG7f+Tuf5H0K0mLW9BH23P3PZI+O2PzYklbsutbNPLD0nQVemsL7j7o7m9l109J+mqZ8ZY+dom+mqIVYe+WdHjU7QG113rvLul3ZrbPzHpb3cwYLnX3QWnkh0fStBb3c6aqy3g30xnLjLfNY1fP8ud5tSLsYy0l1U7zfwvd/TuSbpF0f/Z0FbWpaRnvZhljmfG2UO/y53m1IuwDkmaOuj1D0tEW9DEmdz+aXR6X9KrabynqY1+toJtdHm9xP3/VTst4j7XMuNrgsWvl8uetCPteSbPM7JtmNkHSEkn9LejjLGZ2UfbCiczsIknfU/stRd0vaXl2fbmkHS3s5WvaZRnvSsuMq8WPXcuXP3f3pn9JulUjr8j/r6R/aUUPFfq6UtJ/ZV/vtbo3SVs18rTu/zTyjGiFpL+VtEvSB9llZxv19qKkdyW9o5FgTW9Rb/+okT8N35G0P/u6tdWPXaKvpjxunC4LBMEZdEAQhB0IgrADQRB2IAjCDgRB2IEgCDsQxP8DpZZLbZhMhMcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "digit0=3\n",
    "digit1=7\n",
    "x_bin_train=x_train[np.logical_or(y_train==digit0,y_train==digit1)]\n",
    "y_bin_train=y_train[np.logical_or(y_train==digit0,y_train==digit1)]\n",
    "\n",
    "x_bin_test=x_test[np.logical_or(y_test==digit0,y_test==digit1)]\n",
    "y_bin_test=y_test[np.logical_or(y_test==digit0,y_test==digit1)]\n",
    "\n",
    "print(\"The first training datapoint now is: \\n\")\n",
    "plt.imshow(x_bin_train[0],cmap=plt.cm.gray_r)\n",
    "plt.show()\n",
    "print(y_bin_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n",
    "\n",
    "**a)** Learn different neural network models by varying the network architecture (i.e. number of layers and neurons in each layer) and the type of network (i.e. fully connected vs. convolutional). For each configuration, determine the time it takes to learn the model, and the accuracy on the test data. *Caution*: for some configurations, learning here can take a little while (several minutes).\n",
    "\n",
    "**b)** Inspect some misclassified cases. Do they correspond to hard to recognize digits (also for the human reader)? (Hint: you can e.g. use the numpy where() function to extract the indices of the test cases that were misclassified: `misclass = np.where(test != predictions)` )\n",
    "\n",
    "**c)** How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task (e.g., distinguishing '1' and '0'), or a more difficult one (e.g., '4' vs. '5'). \n",
    "\n",
    "**d)** Identify one or several good configurations that give a reasonable combination of accuracy and runtime. Use these configurations to perform a full classification of the 10 classes in the original dataset (after split into train/test). Using `sklearn.metrics.confusion_matrix` you can get an overview of all combinations of true and predicted labels (see p. 298-299 in Müller & Guido). What does this tell you about which digits are easy, and which ones are difficult to recognize, and which ones are most easily confused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cheating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate the capability of the neural networks to find a good model, knowing that a very accurate model exists. For this, we add some 'cheat info' to our data: we replace the first pixel value in the data matrix by a digit that simply contains a 0/1 encoding of the actual class label: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding cheating information to the training data:\n",
    "cheatcol_train=np.array(y_bin_train) #making a copy of the original target array\n",
    "cheatcol_train[cheatcol_train==digit0]=0  #re-coding the two classes as 0s and 1s\n",
    "cheatcol_train[cheatcol_train==digit1]=1\n",
    "x_bin_cheat_train = np.copy(x_bin_train)\n",
    "x_bin_cheat_train[:,0,0] = cheatcol_train.reshape(len(cheatcol_train))\n",
    "\n",
    "#adding cheating information to the testing data:\n",
    "cheatcol_test=np.array(y_bin_test) #making a copy of the original target array\n",
    "cheatcol_test[cheatcol_test==digit0]=0  #re-coding the two classes as 0s and 1s\n",
    "cheatcol_test[cheatcol_test==digit1]=1\n",
    "x_bin_cheat_test = np.copy(x_bin_test)\n",
    "x_bin_cheat_test[:,0,0] = cheatcol_test.reshape(len(cheatcol_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks should, in principle, be able to construct a 100% accurate classifier for this data: we only have to 'learn' that only the first pixel in the data matters. \n",
    "\n",
    "**e)** Describe, briefly, how the weights of a (fully connected) network (with arbitrary number of layers) would have to be set, so that the resulting model is 100% accurate on this cheating data. This part of the exercise does not involve any Python code. Just give your answer in a short text.\n",
    "\n",
    "**f)** Investigate how the accuracy improves in practice on this new dataset. Do you achieve 100% accuracy on the test set? If not, try to change the encoding in the cheat column: instead of representing digit1 with a 1, use a larger number, e.g. 250. Does that help? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data preparation, exploration and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to investigate student dropout based on the dataset \"churn2000.cvs\". This is a real dataset, and there is no single \"correct\" way to use it (however, there are several wrong ones!). Your exercise is to explore one or more possible usecases, and document the one(s) you find the most fruitful/interesting.  Your work should probably include the steps below:\n",
    "\n",
    "- An investigation of the data, using e.g. FACETs, Pandas, and/or whatever other tools you prefer. Can you find any interesting correlations? Are there problematic features or rows in the dataset?\n",
    "- Handle missing data and possible outliers (in each case, consider what you want to do: Remove row? Remove column? Insert custom value?).\n",
    "- Normalize/bin/create dummy variables where relevant. \n",
    "- Determine what you would like to predict, i.e. choose your target variable. Try formulating a specific usecase for your experiment (e.g. \"Given a students perfomance in high school and first semester, what is the probability that he/she churns in the 2. semester?\")\n",
    "- Train a neural network and at least one other algorithm on the data. Find a good set of hyperparameters for each model. Do you think a neural network is suitable for this kind of problem? Why/why not?\n",
    "- What features seem to be important for predicting whether a student is likely to drop out?\n",
    "\n",
    "Warning: Make sure you carefully consider what information is available at the time where a prediction is to be made - for example, it doesn't make any sense to try to predict if a student churns in semester 1, if you include a feature which tells that this student churned in semester 2!  So depending on your specific usecase, you should probably remove some columns and/or rows before you train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can carry out this part of the project either in a jupyter notebook or in a google colab notebook - the latter option allows you to easily use the GPU, rather than the CPU, for training, which is MUCH faster! See these videos for how to get started with google colab notebooks: https://www.youtube.com/watch?v=inN8seMm7UI (for getting started) and https://www.youtube.com/watch?v=PitcORQSjNM (for using the GPU). At the end, you can simply merge the generated pdfs when you submit your project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this final exercise, we are going to explore the concept of reinforcement learning by training a neural network to play a game of your own choice.\n",
    "\n",
    "We will use the environments provided in OpenAI Gym https://gym.openai.com/ and also documented here: https://github.com/openai/gym/wiki\n",
    "\n",
    "Instead of implementing Q-learning yourselves, you are welcome to simply run a pre-coded example.\n",
    "You can look here for inspiration: https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb \n",
    "\n",
    "In the exercise, toy around with the parameters, and see if you can optimize learning. You should also explain the basics of what is going on. Include some pictures of the animation, and report your score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Give a short description of the rules and incentives in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Run the code (either directly in the notebook or on Colab), and show how the training progresses, for example by taking screenshots and including them as pictures in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Give a short description of what is going on in the code. This should include a description of the neural network, the inputs and outputs to the neural network, and the process of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Report the highest score your network was able to obtain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
