{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning</h1><h2 align=\"center\" style=\"margin:10px\">Assignment 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student names and numbers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignments below should be solved and documented as a mini-project that will form the basis for the\n",
    "examination. When solving the exercises it is important that you\n",
    "\n",
    "  * document all relevant results and analyses that you have obtained/performed during the exercises\n",
    "  * try to relate your results to the theoretical background of the methods being applied.\n",
    "\n",
    "Feel free to add cells if you need to. The easiest way to convert to pdf is to save this notebook as .html (File-->Download as-->HTML) and then convert this html file to pdf. You can also export as pdf directly, but here you need to watch your margins as the converter will cut off your code (i.e. make vertical code!).\n",
    "\n",
    "Last, but not least:\n",
    "* Looking for an overview of the markdown language? The cheat sheet <a href=\"https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\">here</a> might help.\n",
    "* For most of the Python specific components of the exercises, you should not need constructs beyond those that are already included in the notebooks on the course's web-page (still you should not feel constrained by these, so feel free to be adventurous). You may, however, need to consult the documentation for some of the methods supplied by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries here\n",
    "import mglearn\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold= sys.maxsize)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy as scipy\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDB-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv('IMDB_dataset/reviews.txt', header=None)\n",
    "labels = pd.read_csv('IMDB_dataset/labels.txt', header=None)\n",
    "Y = np.array((labels=='positive').astype(np.int_)).ravel()\n",
    "print(type(reviews))\n",
    "print(reviews[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(reviews[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preproccesing \n",
    "#1. Punctuation removal\n",
    "\n",
    "import string \n",
    "for idx, rev in enumerate(reviews[0]):\n",
    "    reviews[0][idx] = rev.translate(str.maketrans('', '', string.punctuation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reviews.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Removal of stop_words\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "for idx, rev in enumerate(reviews[0]):\n",
    "    reviews[0][idx] = remove_stopwords(rev)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reviews.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\spiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\spiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#3. Lemmatisation of reviews \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lmtizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    " \n",
    " \n",
    "for idx, rev in enumerate(reviews[0]):\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(rev)) \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:  \n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:  \n",
    "            lemmatized_sentence.append(lmtizer.lemmatize(word, tag))\n",
    "    reviews[0][idx]= \" \".join(lemmatized_sentence)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(reviews.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Tokenizing words\n",
    "vect = TfidfVectorizer(max_features=10000, stop_words= stopwords.words('english'))\n",
    "transformed_reviews = vect.fit_transform(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(transformed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the dataset into a dataframe for feature extraction\n",
    "\n",
    "import scipy.sparse\n",
    "dataFrame = pd.DataFrame.sparse.from_spmatrix(transformed_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training-val set:18750\n",
      "Size of test set:6250\n",
      "AAAAAA 6250\n"
     ]
    }
   ],
   "source": [
    "Y = to_categorical(Y,num_classes=2)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(dataFrame.to_numpy(), Y, random_state=42)\n",
    "print(\"Size of training-val set:{}\".format(X_trainval.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))\n",
    "print(\"AAAAAA {}\".format(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "150/150 [==============================] - 2s 13ms/step - loss: 0.6847 - accuracy: 0.5861 - val_loss: 0.6696 - val_accuracy: 0.6755\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.6397 - accuracy: 0.7255 - val_loss: 0.5954 - val_accuracy: 0.7888\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5324 - accuracy: 0.7864 - val_loss: 0.4579 - val_accuracy: 0.8304\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4863 - accuracy: 0.7607 - val_loss: 0.4828 - val_accuracy: 0.7405\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.4175 - accuracy: 0.8097 - val_loss: 0.3615 - val_accuracy: 0.8451\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4146 - accuracy: 0.8081 - val_loss: 0.5249 - val_accuracy: 0.7197\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.3705 - accuracy: 0.8353 - val_loss: 0.3263 - val_accuracy: 0.8661\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.3252 - accuracy: 0.8592 - val_loss: 0.3193 - val_accuracy: 0.8664\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.3227 - accuracy: 0.8633 - val_loss: 0.3687 - val_accuracy: 0.8325\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.2916 - accuracy: 0.8794 - val_loss: 0.3053 - val_accuracy: 0.8717\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2967 - accuracy: 0.8729 - val_loss: 0.3017 - val_accuracy: 0.8731\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2652 - accuracy: 0.8902 - val_loss: 0.4363 - val_accuracy: 0.8043\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2635 - accuracy: 0.8909 - val_loss: 0.3524 - val_accuracy: 0.8501\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.2664 - accuracy: 0.8901 - val_loss: 0.3047 - val_accuracy: 0.8707\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.2336 - accuracy: 0.9035 - val_loss: 0.2946 - val_accuracy: 0.8792\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2524 - accuracy: 0.8945 - val_loss: 0.4098 - val_accuracy: 0.8259\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2357 - accuracy: 0.9063 - val_loss: 0.6194 - val_accuracy: 0.7349\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2176 - accuracy: 0.9107 - val_loss: 0.3457 - val_accuracy: 0.8557\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2110 - accuracy: 0.9151 - val_loss: 0.2976 - val_accuracy: 0.8837\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2199 - accuracy: 0.9101 - val_loss: 0.6501 - val_accuracy: 0.7355\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2047 - accuracy: 0.9215 - val_loss: 0.3164 - val_accuracy: 0.8696\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1860 - accuracy: 0.9284 - val_loss: 0.5969 - val_accuracy: 0.7560\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2160 - accuracy: 0.9135 - val_loss: 0.3825 - val_accuracy: 0.8381\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2048 - accuracy: 0.9186 - val_loss: 0.4351 - val_accuracy: 0.8213\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1622 - accuracy: 0.9370 - val_loss: 0.4044 - val_accuracy: 0.8344\n"
     ]
    }
   ],
   "source": [
    "#Now we define and train the neural network:\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=10000)) # Each new line is a new layer! The first layer is called the input-layer\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax')) \n",
    "\n",
    "sdg = optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sdg, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_trainval, y_trainval,validation_split=0.2, epochs=15, batch_size=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 2s 3ms/step - loss: 0.2331 - accuracy: 0.9027\n",
      "Accuracy on training data: [0.2331491857767105, 0.9027199745178223]\n",
      "196/196 [==============================] - 1s 3ms/step - loss: 0.3895 - accuracy: 0.8509\n",
      "Accuracy on training data: [0.3895110487937927, 0.8508800268173218]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training data: {}\".format(model.evaluate(X_trainval, y_trainval)))\n",
    "print(\"Accuracy on training data: {}\".format(model.evaluate(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000164819C7EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.23457369 0.76542634]]\n"
     ]
    }
   ],
   "source": [
    "# review1 = \"There were two interesting look back reviews that came out in 2015 that are worth checking out - just search for <<review terminator sarah connor chronicles>> and check out the retrospectives in The Guardian and on IGN. The IGN retrospective is especially valuable due to the inclusion of video cast interviews that don't show up on YouTube - there are a lot of insights as to how the principle actors viewed their characters and where they hoped the plot and reveals would go. Ultimately the writer Josh Friedman did give us fans what we wanted, just not in the definitive manner the actors hoped for their characters. I think it is better that way - showing us and let us draw our own conclusions, instead of laying it out in black and white. But, yeah, for you Jameron shippers, you were right, the seeds were there all along and the final episode is a real heartbreaker.\"\n",
    "review1 = \"One of the best stand alone TV Sci-Fi shows ever, joining Firefly on the list of <<why ever did they cancel that >>? Perhaps the best answer is that it is a miracle it was made in the first place, given that cancer survivor Josh Friedman produced a death obsessed, stylish, inventive and often very funny TV show with more depth and sophistication than the dying film franchise. A Hollywood insider needs to write the real back story to this show: how it suited Warner Brothers to have what was, in effect, an extended commercial on TV for the upcoming reboot of the movie franchise, what transpired between Warner and Fox, and why Fox started to pull the plug on the cash. There are episodes in season two that were clearly made with everyone concerned expecting to be fired by the end of the day. The production values dropped as the budget dried up, but the excellent acting, production and cinematography did not. An unfortunate casualty of the writers' strike, the show lost momentum after its truncated first season. Neflix, reboot it, please....\"\n",
    "\n",
    "review1 = review1.translate(str.maketrans('', '', string.punctuation))\n",
    "review1 = remove_stopwords(review1)\n",
    "\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(review1)) \n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:  \n",
    "    if tag is None:\n",
    "         lemmatized_sentence.append(word)\n",
    "    else:  \n",
    "        lemmatized_sentence.append(lmtizer.lemmatize(word, tag))\n",
    "review1= \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "review1 = vect.transform([review1])\n",
    "review1 = pd.DataFrame.sparse.from_spmatrix(review1).to_numpy()\n",
    "\n",
    "prediction = model.predict(review1)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. (See an example of how to do this in chapter 7 of \"Muller and Guido\"). Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**d)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**e)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**h)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we consider the famous MNIST dataset, which is loaded below. The dataset consists of 70000 handwritten digits 0-9 at a resolution of 28x28 pixels. In the cell below, the dataset is loaded and split into 60000 traning and 10000 testing images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code-snippet below can be used to see the images corresponding to individual digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = 1\n",
    "\n",
    "plt.imshow(x_train[index],cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little bit simpler (and faster!), we can extract from the data binary subsets, that only contain the data for two selected digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first training datapoint now is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "digit0=3\n",
    "digit1=7\n",
    "x_bin_train=x_train[np.logical_or(y_train==digit0,y_train==digit1)]\n",
    "y_bin_train=y_train[np.logical_or(y_train==digit0,y_train==digit1)]\n",
    "\n",
    "x_bin_test=x_test[np.logical_or(y_test==digit0,y_test==digit1)]\n",
    "y_bin_test=y_test[np.logical_or(y_test==digit0,y_test==digit1)]\n",
    "\n",
    "print(\"The first training datapoint now is: \\n\")\n",
    "plt.imshow(x_bin_train[0],cmap=plt.cm.gray_r)\n",
    "plt.show()\n",
    "print(y_bin_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n",
    "\n",
    "**a)** Learn different neural network models by varying the network architecture (i.e. number of layers and neurons in each layer) and the type of network (i.e. fully connected vs. convolutional). For each configuration, determine the time it takes to learn the model, and the accuracy on the test data. *Caution*: for some configurations, learning here can take a little while (several minutes).\n",
    "\n",
    "**b)** Inspect some misclassified cases. Do they correspond to hard to recognize digits (also for the human reader)? (Hint: you can e.g. use the numpy where() function to extract the indices of the test cases that were misclassified: `misclass = np.where(test != predictions)` )\n",
    "\n",
    "**c)** How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task (e.g., distinguishing '1' and '0'), or a more difficult one (e.g., '4' vs. '5'). \n",
    "\n",
    "**d)** Identify one or several good configurations that give a reasonable combination of accuracy and runtime. Use these configurations to perform a full classification of the 10 classes in the original dataset (after split into train/test). Using `sklearn.metrics.confusion_matrix` you can get an overview of all combinations of true and predicted labels (see p. 298-299 in MÃ¼ller & Guido). What does this tell you about which digits are easy, and which ones are difficult to recognize, and which ones are most easily confused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cheating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate the capability of the neural networks to find a good model, knowing that a very accurate model exists. For this, we add some 'cheat info' to our data: we replace the first pixel value in the data matrix by a digit that simply contains a 0/1 encoding of the actual class label: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding cheating information to the training data:\n",
    "cheatcol_train=np.array(y_bin_train) #making a copy of the original target array\n",
    "cheatcol_train[cheatcol_train==digit0]=0  #re-coding the two classes as 0s and 1s\n",
    "cheatcol_train[cheatcol_train==digit1]=1\n",
    "x_bin_cheat_train = np.copy(x_bin_train)\n",
    "x_bin_cheat_train[:,0,0] = cheatcol_train.reshape(len(cheatcol_train))\n",
    "\n",
    "#adding cheating information to the testing data:\n",
    "cheatcol_test=np.array(y_bin_test) #making a copy of the original target array\n",
    "cheatcol_test[cheatcol_test==digit0]=0  #re-coding the two classes as 0s and 1s\n",
    "cheatcol_test[cheatcol_test==digit1]=1\n",
    "x_bin_cheat_test = np.copy(x_bin_test)\n",
    "x_bin_cheat_test[:,0,0] = cheatcol_test.reshape(len(cheatcol_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks should, in principle, be able to construct a 100% accurate classifier for this data: we only have to 'learn' that only the first pixel in the data matters. \n",
    "\n",
    "**e)** Describe, briefly, how the weights of a (fully connected) network (with arbitrary number of layers) would have to be set, so that the resulting model is 100% accurate on this cheating data. This part of the exercise does not involve any Python code. Just give your answer in a short text.\n",
    "\n",
    "**f)** Investigate how the accuracy improves in practice on this new dataset. Do you achieve 100% accuracy on the test set? If not, try to change the encoding in the cheat column: instead of representing digit1 with a 1, use a larger number, e.g. 250. Does that help? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data preparation, exploration and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to investigate student dropout based on the dataset \"churn2000.cvs\". This is a real dataset, and there is no single \"correct\" way to use it (however, there are several wrong ones!). Your exercise is to explore one or more possible usecases, and document the one(s) you find the most fruitful/interesting.  Your work should probably include the steps below:\n",
    "\n",
    "- An investigation of the data, using e.g. FACETs, Pandas, and/or whatever other tools you prefer. Can you find any interesting correlations? Are there problematic features or rows in the dataset?\n",
    "- Handle missing data and possible outliers (in each case, consider what you want to do: Remove row? Remove column? Insert custom value?).\n",
    "- Normalize/bin/create dummy variables where relevant. \n",
    "- Determine what you would like to predict, i.e. choose your target variable. Try formulating a specific usecase for your experiment (e.g. \"Given a students perfomance in high school and first semester, what is the probability that he/she churns in the 2. semester?\")\n",
    "- Train a neural network and at least one other algorithm on the data. Find a good set of hyperparameters for each model. Do you think a neural network is suitable for this kind of problem? Why/why not?\n",
    "- What features seem to be important for predicting whether a student is likely to drop out?\n",
    "\n",
    "Warning: Make sure you carefully consider what information is available at the time where a prediction is to be made - for example, it doesn't make any sense to try to predict if a student churns in semester 1, if you include a feature which tells that this student churned in semester 2!  So depending on your specific usecase, you should probably remove some columns and/or rows before you train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can carry out this part of the project either in a jupyter notebook or in a google colab notebook - the latter option allows you to easily use the GPU, rather than the CPU, for training, which is MUCH faster! See these videos for how to get started with google colab notebooks: https://www.youtube.com/watch?v=inN8seMm7UI (for getting started) and https://www.youtube.com/watch?v=PitcORQSjNM (for using the GPU). At the end, you can simply merge the generated pdfs when you submit your project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this final exercise, we are going to explore the concept of reinforcement learning by training a neural network to play a game of your own choice.\n",
    "\n",
    "We will use the environments provided in OpenAI Gym https://gym.openai.com/ and also documented here: https://github.com/openai/gym/wiki\n",
    "\n",
    "Instead of implementing Q-learning yourselves, you are welcome to simply run a pre-coded example.\n",
    "You can look here for inspiration: https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb \n",
    "\n",
    "In the exercise, toy around with the parameters, and see if you can optimize learning. You should also explain the basics of what is going on. Include some pictures of the animation, and report your score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Give a short description of the rules and incentives in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Run the code (either directly in the notebook or on Colab), and show how the training progresses, for example by taking screenshots and including them as pictures in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Give a short description of what is going on in the code. This should include a description of the neural network, the inputs and outputs to the neural network, and the process of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Report the highest score your network was able to obtain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
